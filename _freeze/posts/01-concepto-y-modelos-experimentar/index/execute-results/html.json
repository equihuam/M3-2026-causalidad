{
  "hash": "5c07de2c3d2775a8b0c5113ef473f42b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Conceptos y Modelos en la experimentación\"\nauthor: [\"Miguel Equihua\"]\ndate: \"2026-01-26 10:00:00\"\nlang: es\ndraft: false\ncategories: [clase]\n\n\nformat: html\nengine: knitr \nfilters:\n  - webr\n\ntheme: \n    dark: Sandstone\n    \nwebr:\n  packages: ['gt', 'dplyr', 'tidyr']\n  \nbibliography: references.bib\n---\n\n\n::: {.cell}\n\n:::\n\n\n# Un cuento de Vampiros y Ciencia\n\n![](images/vampiro-dracula.png){width=\"300\"}\n\nMcElreath (2020) presenta en su libro el hipotético ejemplo de *la\nprueba de sangre para vampirismo*. Propone que hay un análisis de sangre\nque detecta correctamente 95% de las veces, la afiliación de un\nindividuo al linaje del conde Drácula y los inmortales vampiros. En\nnotación matemática:\n\n$$\n{Pr(resultado~ positivo~de~la~prueba|vampiro) = 0.95}\n$$\n\nEs una prueba muy precisa, casi siempre identificando *vampiros* reales.\nSin embargo, también comete errores y produce falsos positivos. Es así\nque el uno por ciento de las veces diagnostica incorrectamente a los\n*simples mortales* como *vampiros*:\n\n$$\n{Pr(resultado~positivo~de~la~prueba|mortal) = 0.01}\n$$\n\nLa última pieza de información que necesitamos es saber que los\n*vampiros* en realidad son bastante raros. Sólo el 0.1% de la población\nlo es, lo que implica:\n\n$$\n{Pr(vampiro) = 0.001}\n$$\n\nA partir de este conocimiento científico, supongamos que un amigo da\npositivo en el test de vampirismo.\n\n::: {.callout-tip title=\"¿Cuál es la probabilidad de que sea un inmortal chupasangre\" collapse=\"true\"}\nEl enfoque de investigación formal empezaría por usar el *teorema de\nBayes* para deducir la probabilidad ${Pr(vampiro|positivo)}$, lo que en\ncierta forma implica \"invertir la probabilidad\", pues lo que ahora\nsabemos es el valor de ${Pr(positivo|vampiro)}$. El cálculo puede\npresentarse como:\n\n$$\n\\Pr(vampiro|positivo) = \\frac{Pr(positivo|vampiro) \\times Pr(vampiro)}{Pr(positivo)}\n$$\n\nen donde ${Pr(positivo)}$ es la probabilidad promedio de los resultados\npositivos de la prueba, es decir,\n\n$$\nPr(positivo) = Pr(positivo|vampiro)\\times Pr(vampiro) + Pr(positivo|mortal) \\times ({1 − Pr(vampiro)}) \n$$\n:::\n\nTodo esto lo podemos hacer en **R**.\n\nPrimero tomamos nota de lo que ya sabemos a partir del enunciado\nanterior, ¡*a priori*! Por favor proporciona los datos requeridos en el\nrecuadro siguiente.\n\n```{webr-r}\n\npr <- tribble(~concepto,                      ~momento,    ~valor,\n              \"prueba positiva en vampiros\", \"al inicio\",   NA,\n              \"prueba positiva en mortales\", \"al inicio\",   NA,\n              \"ser un vampiro en Xalapa\",    \"al inicio\",   NA) \n\n```\n\nSi queremos verificar los datos podemos verlos simplemente anotando el\nnombre de la tabla, en este caso: `pr`\n\n```{webr-r}\npr\n```\n\nSi quisieras presentarla como una bonita tabla en tu reporte será mejor\nusar algo como`flextable` o `gt`\n\n```{webr-r}\n#| results: as-is\n\npr |> \n  gt() |> \n  cols_label(concepto = \"Concepto\", \n             momento = \"¿Cuándo lo sé?\", \n             valor   = \"Probabilidad\") |> \n  fmt_percent(valor, decimals = 1) |> \n  as_raw_html(inline_css = FALSE)\n\n```\n\nTomamos la fórmula de Bayes para *invertir la probabilidad*, pues\nqueremos saber qué está pasando cuando tenemos la fortuna de toparnos\ncon un resultado positivo en la prueba de sangre:\n\n$$\nPr(positivo|vampiro)\n$$\n\nEsto equivale a preguntarnos, dado que ya vimos el resultado científico\nque significa la prueba de sangre, ¿será vampiro el sujeto de quien se\nobtuvo esa muestra?:\n\n$$\nPr(vampiro|positivo)\n$$\n\n```{webr-r}\n#| results: as-is\n\nPr_positivo <- pr$valor[pr$concepto == \"prueba positiva en vampiros\"] * \n               pr$valor[pr$concepto == \"ser un vampiro en Xalapa\"] +     # vampiro da positivo\n  \n               pr$valor[pr$concepto == \"prueba positiva en mortales\"] * \n               (1 - pr$valor[pr$concepto == \"ser un vampiro en Xalapa\"]) # Mortal da positivo\n\npr |> \n  rows_append(\n    tibble(concepto = \"ser vampiro con prueba positiva\",\n           momento = \"al conocer la prueba positiva\",\n           valor = pr$valor[pr$concepto == \"prueba positiva en vampiros\"] *\n           pr$valor[pr$concepto == \"ser un vampiro en Xalapa\"] / \n           Pr_positivo)) |> \n  gt() |> \n  cols_label(concepto = \"Concepto\", \n             momento = \"¿Cuándo lo sé?\", \n             valor   = \"Probabilidad\") |> \n  fmt_percent(valor, decimals = 1)\n\n\n```\n\nPor lo tanto, esta es la probabilidad de que el amigo sea en realidad un\nvampiro.\n\n::: {.callout-tip title=\"¿Encuentras este resultado afin o contrario a lo que pensabas antes de hacer los cálculos?\"}\n![](images/questions-1922477_1280.jpg){width=\"30%\"}\n:::\n\nEste es un resultado muy importante. Exactamente así, o algo muy\nparecido, es el procedimiento que se sigue en la realiad al efectuar\npruebas diagnósticas: *las pruebas de PCR*, *antígeno* o *anticuerpos\npara SarsCov-2*, *la prueba del VIH* la del *DNA en un perfil criminal*\ny por supuesto la *prueba de significancia estadística*.\n\nQuizás ayude a mejorar la intuición que tenemos de las cosas el\nconsiderar que siempre que la condición de interés sea muy rara,\ndesarrollar una *prueba excelente*, capaz de diagnosticar bien todos los\ncasos verdaderos (aunque invitablemente produzca también algunos falsos\npositivos), puede no ser garantía suficiente de que un resultado\npositivo en general conlleve información categóricamente significativa.\n\nLa razón es que usualmente resulta inevitable tener **falsos positivos**\ny por simple aritmética, esos casos serán la mayoría de los resultados\nque tendremos, incluso si todos los *verdaderos positivos* fueran\n*detectados correctamente*.\n\nLa regla de Bayes es útil aquí, aunque, como dice McElreath, no es la\núnica manera de razonar en este caso. Podríamos pensar que la ecuación\nque usamos aquí salio de la nada, aunque quizás la recuerdes de algún\ncurso previo, de alguna charla interesante por ahí o incluso de lo que\nviste con Rosario ¡hace unas semanas!\n\nQuizás el ejemplo puede verse en forma más intuitiva utilizando otra\nnarrativa para comprender lo que está ocurriendo. Digamos que en lugar\nde informar sobre las probabilidades, como antes, te digo lo siguiente:\n\n1.  En una población de 100,000 personas, 100 son *vampiros*.\n2.  De los 100 que son vampiros 95 darán positivo en la prueba de\n    vampirismo.\n3.  De los 99,900 *simples mortales* restantes, 999 darán positivo a la\n    prueba de vampirismo.\n\nAhora piensa en esto, si hacemos pruebas a las 100,000 personas, ¿qué\nproporción de los que dan positivo en las pruebas de vampirismo son\nrealmente *vampiros*?\n\nMuchas personas, aunque ciertamente no todas, encuentran esta forma de\ncontar la historia mucho más fácil. Sigamos por este camino.\n\n::: {.callout-tip title=\"¿Qué tal si contamos el número de personas que dan positivo?\" collapse=\"true\"}\nAnota los valores que consideres adecuados en el recuadro\n\n```{webr-r}\n\n\n\n```\n\nDe este total de pruebas positivas, sabemos que sólo 95 provienen\nverdaderamente de vampiros, lo que nos lleva sencillamente a pregutarnos\nsobre la fracción que significa esa cantidad de vampiros entre la\ntotalidad de pruebas positivas.\n\n```{webr-r}\n\n\n```\n\n¿Qué valor obtuviste?\n\nSi todo marcho bien deberías tener exactamente la misma respuesta que\npor el camino bayesiano que encontramos antes. L diferencia es que en\neste caso, no tuvimos que recordar la ´\"formula mágica\" de Bayes, nada\nmás tuvimos que imgainar la situación de otra manera y simplemente\ncontar y pensar con calma.\n\nEsta forma de presentar el problema mediante el \"conteo de los actores\"\nen lugar de recurrir a probabilidades, suele denominarse *formato de\nfrecuencia* o *frecuencias naturales*. En odo caso, lo importante es\napreciar que el razonamiento puede beneficiarse de adoptar formas\nfrugales e intuitivas [@gigerenzer2017; @hicks2012].\n\nLas razones propuestas para explicar el por qué el *formato de\nfrecuencia* ayuda a la gente a intuir el enfoque correcto siguen siendo\npolémicas. Podría ser que de entrada sólo podemos encontrarnos con\nconteos en el mundo real. Quizás sea cierto que nadie ha visto nunca una\nprobabilidad andando por ahí. Independientemente de la explicación de\neste fenómeno, podemos explotarlo en muchas situaciones cotidianas.\n\n:::\n\nLos eventos muestreados en el análisis de las distribuciones de\nprobabilidades de modelos estadísticos en algún análisis de datos, son\nlos valores de los parámetros. La mayoría de los parámetros no tienen\nuna \"materialización\" empírica exacta.\n\nEl formalismo bayesiano trata las distribuciones de los parámetros como\nuna plausibilidad relativa, no como un proceso aleatorio que ocurre en\nel mundo físico. En cualquier caso, la aleatoriedad es siempre una\npropiedad de la información, nunca del mundo real. Para continuar\necplorando esta idea puede interesarte los textos de\n[Gigerenzer](https://onlinelibrary.wiley.com/authored-by/Gigerenzer/Gerd?pageSize=20&startPage=1))\n\n\n## Estadística para mejorar la ciencia\n\n::: {.callot-tip title=\"¿Podemos confiar en que la estadística nos proteja de la mala ciencia?\"}\nEl ejemplo del vampirismo que acabamos de ver tiene la misma estructura\nlógica que muchos problemas de *detección* considerando que:\n\n1.  Hay algún estado binario al que no tenemos acceso.\n2.  Observamos un indicio imperfecto del estado oculto.\n3.  (Deberíamos/podríamos) usar el teorema de Bayes para deducir\n    lógicamente el impacto del indicio en nuestra incertidumbre (aunque\n    [ve lo que salió en el\n    periódico](http://www.theguardian.com/law/2011/oct/02/formula-justice-bayes-theorem-miscarriage))\n\nLa inferencia científica puede enmarcase en términos similares:\n\n1.  Una hipótesis es verdadera o falsa, pero no podemos saberlo;\n2.  Obtenemos un indicio estadístico de la **falsedad** de la hipótesis;\n3.  Debemos/podemos utilizar el teorema de Bayes para deducir\n    lógicamente el impacto del indicio en el estado de la hipótesis.\n\nEs el tercer paso el que casi nunca se hace. Sin debatir por lo pronto\nsi debemos o no usar a Bayes, consideremos por un momento la idea como\nun ejemplo de juguete.\n:::\n\n### Resultados posibles de un estudio científico\n\n#### Probabilidad de *acertar*\n\nSupongamos que la probabilidad de obtener un hallazgo positivo, cuando\nla hipótesis postulada es cierta, es\n${Pr(señal~detectada|verdadero) =Pr(H|V) = 0.95}$.\n\nEse es lo que se suele llamar **la potencia de la prueba**.\n\n#### Probabilidad de *errar*\n\nSupongamos que la probabilidad de un hallazgo positivo, cuando una\nhipótesis es falsa, es ${Pr(señal~detectada|falso) = Pr(H|F) = 0.05}$.\n\nEsa es la tasa de falsos positivos, se trata del, digamos 5%, de la\n**prueba de significancia** que usamos convencionalmente.\n\n#### ¿Cuál será el estado real del mundo?\n\nFinalmente, tenemos que establecer la tasa base con la que ocurren las\nhipótesis que son verdaderas. Supongamos, por ejemplo, que 1 de cada 100\nhipótesis resulta ser verdadera. Entonces\n\n${Pr(verdadero) = P(V) = 0.01}$.\n\nEn realidad nadie conoce este valor ni se ve posible conocerlo, pero la\nhistoria de la ciencia sugiere que es pequeño.\n\n#### Probabilidad de haber acertado con una hipótesis\n\nPara averiguar esto, calculamos la componente *a posteriori*:\n\n$$\nPr(detectada|Hipótesis) = \\frac{Pr(Hipótesis|detectada) Pr(detectado)} {Pr(Hipótesis)} = \\\\\n\\\\\n\\frac{Pr(H|V) Pr(V)} {Pr(H|V) Pr(V) + Pr(H|F) Pr(F)}\n\\\\\n$$\n\n```{webr-r}\n\nPr_posterior_hallazgo_cierto <- (0.95 * 0.01) / \n                                   ((0.95 * 0.01) + (0.05 * (1-0.01)))\n\nPr_posterior_hallazgo_cierto\n```\n\nComo podemos ver, al substituir los valores *imaginados*, encontramos\nque la respuesta es aproximadamente ${Pr(V|H) = 0.16}$.\n\nAsí que un resultado positivo corresponde a un 16% de probabilidad de\nque la hipótesis sea cierta.\n\nEste es el mismo fenómeno de baja tasa de base que se aplica en las\npruebas médicas (y también en nuestro ejemplo de *vampiros*).\n\n::: {.callout-warning title=\"¿Podremos mejorar la práctica científica?\"}\n![](images/confundida.png){width=\"50%\"}\n:::\n\n:::: {.callout-note title=\"Experimentos pensados\" collapse=\"true\"}\nUna manera de explorar lo que puede pasar en distintos escenarios,\nasumiendo un razonamiento específico, nos da oportunidad de valorar la\nutilidad de hacer un *escript*, *algoritmo* o *programa*. Así podemos\nautomatizar una tarea repetitiva y potencialmente aburrida para ver las\nimplicaciones de la idea en todo tipo de situaciones. Qizás podríamos\nconsiderarlo semejante a lo qe Einstein llamaba **experimento pensado**.\nVeamos como hacerlo.\n\n```{webr-r}\n\nciencia <- function (pr_pos_cierto = 0.95, pr_cierto = 0.01)\n{\n    pr_falso <- 1 - pr_cierto\n    pr_pos_falso <- 1 - pr_pos_cierto\n\n    pr_cierto_pos <- pr_pos_cierto * pr_cierto / \n                    (pr_pos_cierto * pr_cierto + pr_pos_falso * pr_falso)\n\n    return(pr_cierto_pos)\n}\n```\n\nHemos definido un programa como una *función* en **R**. Esta función\npuede tomar datos y procesarlos de acuerdo con la lógica que le hemos\nespecificado.\n\nAhora podemos experimentar para tener una idea aproximada de lo que está\npasando.\n\n1.  Elegimos una serie de valores de interés (cada uno sería un\n    *escenario*)\n\n```{webr-r}\nvalores_de_interés <- seq(0.9, 0.99, 0.01)\nstr(valores_de_interés)\nhead(valores_de_interés)\n```\n\n2.  Preparamos un espacio en la memoria para anotar los resultados.\n\n```{webr-r}\nresultados <- data.frame(numeric(0), numeric(0))\nfor (pr in valores_de_interés) \n    resultados <- rbind(resultados, c(pr, ciencia(pr, 0.01)))\n\nnames(resultados) <- c(\"p_error\", \"p_post\")\n\n```\n\n3.  Veamos los resultados del experimento con ayuda de una gráfica.\n\n```{webr-r}\n# fondo de las gráficas\npar(bg = \"white\")\n\n# Área de graficación. mar() para el margen. oma() alrededor del margen.\npar(oma=c(2,3,1,2)) # abajo=2, izq=3, arriba=1, der=2\npar(mar=c(4,4,2,2) + 0.7)\n\n# Tamaño de la gráfica\noptions(repr.plot.width = 15, repr.plot.height = 5)\n\n# fondo de las gráficas evita aparrezca un bloque obscurro en su lugar\npar(bg = \"white\")\n\n# Gráfica\nplot(x = 1 - resultados$p_error, y = resultados$p_post,\n     type = \"b\", xlab = \"valor de P\", ylab = \"Pr post detección positiva\", \n     cex = 2, cex.axis  = 1, cex.lab = 2)\n```\n\n![](images/doubt-2137526_1280.png){width=\"10%\"}\n\n::: {.callout-tip title=\"¿Tú que piensas?\" collapse=\"true\"}\nUna investigación muy exigente que reduce la detección de falsos\npositivos a 1%, nos permite llevar la probabilidad posterior de\n*descubrimientos exitosos* hasta 0.5.\n\nApenas tan buena como el lanzamiento de una moneda.\n\nPodemos pensar que lo que hemos hecho hasta aquí es prácticamente un\njuego, pero ¿qué tan cercano podría ser a lo que ocurre en la vida real?\ny si fuera una razonable aproximación ¿a qué nos conduce?\n\n¿Qué conclusiones surgen de esta experiencia?\n\n- [ ] ¿Lo más importante es mejorar la probabilidad de detección base, ${Pr(V)}$?\n- [ ] ¿Qué implicaría y cómo se podría lograr mejorar ${Pr(V)}$?\n- [ ] ¿Es posible hacer que esa probabilidad crezca? \n- [ ] ¿Simpre incrementar el número de veces que se repite el tratamiento (tamaño de muestra?\n- [ ] ¿Sólo aceptar como significativos resultados muy altamente significativos?\n\n:::\n::::\n\n## Crisis de reproducibilidad\n\n::::: columns\n::: {.column width=\"50%\"}\n![](images/Crisis_reproducibilidad.png)\n:::\n\n::: {.column width=\"50%\"}\n![](images/Crisis_reproducibilidad-2.png)\n:::\n:::::\n\n**Fuente:** [Stoddart, C. (2016). Is there a reproducibility crisis in\nscience?. Nature](https://doi.org/10.1038/d41586-019-00067-3)\n\n\\\n\\\n\\\n\n:::: {.callout-tip collapse=\"true\"}\n### ¿Ha mejorado la situación?\n\n::: {layout=\"[[100],[-100], [60,-5, 35]]\"}\nLa falta de reproducibilidad de los experimentos se traduce en artículos\nque son cuestionados y se ven forzados a retirarse. Claro, no es la\núnica razón para retirar un artículo, pero si la más frecuente. A fines\ndel año pasado salió esta noticia en Nature.\n\n[![](images/articulos-retirados.png)](https://www.nature.com/articles/d41586-023-03974-8)\n\n![](images/fabrica-papers.png)\n:::\n::::\n\nSin duda la forma como hemos optado por hacer ciencia está teniendo\nproblemas que debemos afrontar. La cuestión es que podemos o más bien\nque debemos hacer para enfrentarlos productivamente y así producir una\nciencia mejor.\n\n:::: {.callout-tip title=\"Buscar la verdad\" collapse=\"true\"}\n::: {layout=\"[60, -5, 35]\"}\n> La búsqueda de la verdad siempre debería prevalecer sobre el deseo\n> defensivo del ego de tener la razón. Esto no es fácil, porque a la\n> mayoría de la gente le resulta difícil admitir que se equivoca. Y es\n> precisamente por esto por lo que la ciencia resulta tan liberadora.\n> Provee un marco de trabajo para la autocorrección, porque el\n> conocimiento científico siempre es provisional. Un hecho científico\n> aceptado hoy puede ser refutado mañana. Por tanto, el método\n> científico engendra humildad epistemológica.\n\n![](images/saad-gad.jpg)\n:::\n\n[Gad\nSaad](https://www.frontiersin.org/articles/10.3389/fpsyg.2020.579578/full)\n::::\n\n## Preregistro y prepublicación\n\n[Reflexión sobre\nprepublicación](https://blog.scielo.org/es/2020/10/15/son-los-preprints-un-problema-5-formas-de-mejorar-la-calidad-y-credibilidad-de-los-preprints/)\n\n[EcoEvorxiv](https://ecoevorxiv.org/)\n\n[preprints.org](https://www.preprints.org/instructions-for-authors)\n\n[Nature](https://www.nature.com/nature-portfolio/editorial-policies/preprints-and-conference-proceedings)\n\n[Open Science Facility](https://osf.io/preprints)\n\n[Science](https://www.science.org/content/article/preprints-often-make-news-many-people-don-t-know-what-they-are)\n\n\n# El enfoque de modelación\n\nLa propuesta que seguiremos en el **Módulo 3** se basa en traducir\nsituaciones reales en abstracciones que escribimos como modelos\nestadísticos. De esta manera buscamos generar expresiones matemáticas\nque dan cuenta de nuestras concepciones sobre cómo funcionan aspectos\nconcretos de la naturaleza (*proposición causal*). Es así que nos\nplanteamos generar series de números *idealizados por el modelo* que\npodemos comparar con series de números observamos directamente de la\nnaturaleza. Sí estas dos series de números se *parecen lo suficiente*,\nnos *sentiremos con confianza* como para pensar que nuestro modelo es\nútil para describir el comportamiento del segmento de la naturaleza que\nelegimos abstraer.\n\n::: {.callout-tip title=\"¿Podríamos usarlo para intervenir en él?\"}\n![](images/confused-880735_1280.jpg){width=\"50%\"}\n:::\n\nEl modelo que contiene todas las variables explicativas que proponemos\ntinen que ver con el fenómeno que estamos analizando lo llamaremos\n**modelo completo**. Tiene todos los parámetros posibles de acuerdo con\nel diseño del estudio, usualmente al menos un término por cada grupo\nexperimental (tratamiento). En general el método para llegar a un\nsegundo modelo es postular restricciones (supuestos que especifican el\nvalor de particulares) sobre los parámetros del primer modelo.\n\nLas restricciones usualmente son nuestra(s) hipótesis nula(s) y en ese\ncaso las usamos para eliminar algunos parámetros del conjunto usado en\nel modelo completo, equivalente a dar por hecho que el parámetro\ncorrespondiente es 0. Naturalmente llamaremos al modelo resultante\nsimplemente el **modelo restringido** o **modelo reducido**.\n\n::: {.callout-tip title=\"¿Piensas que sería válido restringir un modelo con valores de algún parámetro con un valor que no sea cero?\"}\n![](images/building-1080599_1280.jpg){width=\"50%\"}\n:::\n\nConsideremos el ejemplo siguiente. Se trata de un grupo de hojas a las\nque medimos el contenido de nitrógeno. En este estudio no sabemos ni\nestamos postulando ninguna forma de obtener predicciones distintas para\nsubgrupos de hojas. El modelo correspondiente es:\n\n:::: {.callout-tip title=\"¿Es éste un modelo de regresión simple?\"}\n¿Podrías explicar esta ecuación?\n\n$$\ny_{i} = \\mu + \\varepsilon_{i}\n$$\n\n::: {.callout-important collapse=\"true\"}\n![](images/ANOVA_Modelo_hojas-01.png)\n:::\n::::\n\nUna idea simple para comparar los modelos que acabamos de ver es tomar\nla diferencia en el error asociado con cada modelo. Como esta diferencia\nestará afectada por la escala de medición del fenómeno estudiado,\nconviene *normalizarla*, es decir expresarla como un \\*error relativo\n**para obtenerlo sólo habremos de dividir por el error más pequeño que\npodríamos tener:** la del modelo completo**. Llamarémos a este valor,\nprovisionalmene, el *índice proporcional de error* (**IPE\\*\\*). Un\níndice grande corresponde con modelos que difieren mucho en su *ajuste*\na los datos. Por tanto, estaremos ante la necesidad de elegir alguno de\nlos dos.\n\nAl considerar modelos de distinta complejidad debemos pensar en alguna\nmanera de incorporar ese aspecto en nuestro análisis. Es así que\nllegamos a la noción de *grados de libertad* que la expresamos como el\nnúmero de observaciones menos el número de parámetros en un modelo.\n\nCon todo esto, el cálculo del *índice proporcional de error* (**IPE**)\nse puede formular para eliminar el efecto de la complejidad de los\nmodelos que se comparan. El resultado es este:\n\n$$\nIPE=\\frac{\\frac{(E_{R}-E_{C})}{gl_{C}-gl_{R}}}{\\frac{E_{C}}{gl_{C}}} \\\\\n\\text{ si podemos suponer normalidad en la variable de respuesta }\\\\\n\\sim F_{(gl_{C}-gl_{R}), gl_{C}}\n$$ Así, el estadístico de prueba que hemos construido es el **IPE**. Si\nsuponemos que las variables son adecuadamente descritas por un modelo de\nprobabilidades *normal* (Gaussiano), entones el IPE se distribuirá como\nuna función de probabilidades *F*. Por cierto, si el experimento sólo\ncompara dos medias entonces se cumple que los estadísticos *t* y *F*\nrefieren en última instancia a la misma distribución de probabilidades:\n$F = t^{2}$\n\n::: {.callout-important title=\"Mesaje clave\"}\n-   El análisis de datos *la mayoría de las veces*, se realiza\n    comparando distintos modelos.\n-   Cada modelo corresponde con alguna hipótesis particular.\n-   Los distintos modelos abarcan las varias hipótesis nulas que podrían\n    plantearse, H<sub>0</sub>, pero también pueden incluir cuaquiera de\n    las *hipótesis alternativa*, H<sub>A</sub>, que nos interese\n    valorar.\n-   El procesos de selección de modelos y las caracterísiticas de los\n    modelos que finalmente elijamos serán los protagonistas de la\n    historia que habremos de narrar en el reporte de resultados y\n    discusión de hallazgos.\n:::\n\n```{webr-r}\n\npf(198.50, 1, 2, lower.tail=F)\npt(sqrt(198.50), 2, lower.tail = F)*2\n```\n\n## ANOVA\n\n### Conceptos básicos (modelo de una vía, un criterio, completamente aleatorizado)\n\nHay situaciones en las que la información que tenemos para predecir una\nrespuesta la tenemos en forma cualitativa, incluso la presencia o\nausencia de una categoría. Una variable categórica es una medición\ndiscreta y las clases no tienen ningún orden particular. Por ejemplo,\nconsideremos de nuevo las diferentes especies en los datos de energía\nláctea. Algunas de ellas son simios, mientras que otras son monos del\nNuevo Mundo. Podríamos preguntarnos cómo deberían variar las\npredicciones cuando la especie es un simio en lugar de un mono. El grupo\ntaxonómico es una variable categórica, porque ninguna especie puede ser\nmitad simio y mitad mono (discreción), y no hay ningún sentido en el que\nuno sea más grande o más pequeño que el otro (desorden). Otros ejemplos\ncomunes de variables categóricas son:\n\n-   Sexo: macho, hembra\n-   Estado de desarrollo: lactante, juvenil, adulto\n-   Región geográfica: África, Europa, Melanesia\n\nAlgunos de ustedes ya sabrán que variables como esta, llamadas\nrutinariamente *factores*, pueden ser fácilmente incluidas en los\nmodelos lineales. Pero lo que no resulta tan intuitivo es la forma cómo\nse representan estas variables en un modelo. El ordenador hace todo el\ntrabajo por nosotros, ocultando la maquinaria.\n\nLa hipótesis nula en un análisis de la varianza tipo I común es:\n\n$$\nH_0: m_1 = m_2 = m_3 = ... = m_k\n$$\n\n::: {.callout-tip title=\"¿Cómo es que esta hipótesis se pone a prueba en un ANDEVA?\"}\nPor cierto esta es una prueba \"omnibus\", es decir ¡prueba todo (la\nigualdad de todas las medias) de un jalón!\n:::\n\nPara ver como es que opera el anova veamos el ejemplo que sigue.\nConsidera un solo factor, \"f\", con dos niveles.\n\n```{webr-r}\n\nset.seed(1234) # para fines didácticos: obtener resultados reproducibles \n\nanova.data <- data.frame(y=c(rnorm(7, 5.4, 1), rnorm(7, 10.8, 1)))\nanova.data$f <- factor(rep(c(\"a\", \"b\"), each = 7))  \n\n```\n\nPongamos estos datos en una gráfica simple, sgún el orden en el que\nfueron obtenidas las mediciones. Lo primero que haremos es definir la\ntabla de datos **anova.data**, como espacio de trabajo. Haremos esto con\nla función `attach()`. Esto hace que las variables contenidas en la\ntabla se puedan llamar directamente sin tener que anteponer el nombre de\nla estructura que las contiene. Esto es conveniente, pero si olvidamos\nregresar al espacio general de trabajo, con la función `detach()`,\npodemos encontrarnos con situaciones algo extrañas. En caso de que eso\nocurra, resulta útil la función `search()`, que muestra los espacios de\ntrabajo activos.\n\n```{webr-r}\n\nattach(anova.data)\n\n# fondo de las gráficas\npar(bg = \"white\")\n\nplot(y)\nabline(mean(y), 0)\nfor (i in 1:length(y)) lines (c(i,i), c(mean(y), y[i]))\n```\n\n::: {.callout-tip title=\"¿Qué muestra esta gráfica? ¿a que equivale la suma de los trazos verticales?\" collapse=\"true\"}\n```{webr-r}\n\ndist_tot_y <- sum((y - mean(y))**2)\ndist_tot_y\n```\n:::\n\nAhora incorporemos la información del factor ***f***. Para esto hay que\ncalcular los promedios de \"y\" que corresponden a los niveles de ***f***\n\n```{webr-r}\n\nmeans <- tapply(y, f, mean)\nmeans\n```\n\nGrafiquemos esta nueva estructura de datos.\n\n```{webr-r}\n# fondo de las gráficas\npar(bg = \"white\")\n\nplot(y)\nlines(c(1, 7.5), c(means[1], means[1]))\nlines(c(7.5, 14), c(means[2], means[2]))\nfor (i in 1:7 ) lines (c(i,i), c(means[1], y[i]))\nfor (i in 8:14) lines (c(i,i), c(means[2], y[i]))\n```\n\n::: {.callout-tip title=\"¿Qué muestra esta gráfica? ¿a que equivale la suma de los trazos verticales?\" collapse=\"true\"}\n\n```{webr-r}\n\ndist_tot_s <- sum(m1=sum((y[1:7] - means[1])**2), \n                  m2=sum((y[8:14] - means[2])**2))\n\ndist_tot_s\n```\n:::\n\n### [**Si las dos *medias* fueran iguales ¿cómo compararían estas dos gráficas?**]{style=\"color:GoldenRod\"}\n\n::: {.callout-tip title=\"¿Qué interpretación tiene la diferencia entre las dos sumas mencionadas arriba?\" collapse=\"true\"}\n\nEsta diferencia se asocia con la siguiente gráfica:\n\n```{webr-r}\n# fondo de las gráficas\npar(bg = \"white\")\n\nmodelo <- lm(y ~ f) # ¿qué estoy haciendo aquí?\nplot (y, col = \"red\")\nabline (mean(y), 0)\npoints(predict(modelo), pch = 16, col = \"lightblue\")\nfor (i in 1:14) lines(c(i, i), c(mean(y), predict(modelo)[i]), col = \"gray\")\n```\n\nAhora la suma de estas líneas verticales es\n\n```{webr-r}\n\ndist_tot_s_y <- sum(sum((rep(means[1], 7) - mean(anova.data$y))**2),  \n                    sum((rep(means[2], 7) - mean(anova.data$y))**2))\ndist_tot_s_y\n```\n:::\n\nEstas tres formas de calcular las distancias entre datos y promedios se\nasocia con *fuentes de variación*\n\n1.  variaciones en las observaciones o error total\n2.  variaciones intrínsecas de los sujetos o error residual (componente\n    aleatorio/efecto aleatorio)\n3.  variaciones por efecto del factor causal o error del modelo\n    (componente sistemático/efecto fijo)\n\nEn el cuadro de análisis de varianza se suele etiquetar a los\ncomponentes de error de acuerdo con su *fuente*. Se les acompaña con los\ngrados de libertad, la *suma de cuadrados* de las distancias que mostré\nen las tres gráficas anteriores y luego los llamados *cuadrados medios*.\nPara referencia podemos pedrle a **R** que nos reporte el cuadro de\nANOVA de este modelo.\n\n::: {.callout-tip title=\"Analiza la correspondencia entre los valores y las gráficas que vimos arriba con lo que reporta **R**\" collapse=\"true\"}\n```{webr-r}\n\nanova(modelo)\n\ndist_tot_y\ndist_tot_s\ndist_tot_s_y \n\ndetach(anova.data)\n\n```\n:::\n\n\n## DAG de un diseño experimental simple\n\nUn experimento con asignación de tratamientos en forma completamente al\nazar, garantiza al máximo posile las vías de influencias ocultas o\ninadvertidas. Por diseño, la respuesta de las unidades experimentales,\n*Y*, al tratamiento *T* sólo tiene a la asignación aleatoria *A* como\núnica causa que antecede al tratamiento. Esto lo podemos representar con\nel diagrama acíclico dirigido, **DAG**, siguiente:\n\n$$\n\\fbox{A} \\rightarrow T \\rightarrow Y\n$$\n\nEn este DAG, el marco que rodea a la *A* indica *aleatorización*, y como\nsugiere el diagrama, es la única causa que actua sore el tratamiento. Si\nhubiera una vía de influencia alternativa (*backdoor*), a través de\nalguna tercera variable como podría ser en un caso de germinación de\nsemillas, la luminosidad del sitio o el grado de humendad en el\nsustrato; entonces, la aleatorización no sería el único factor que\ninfluiría sore el tratamiento. El recuadro alrededor de *A* (el proceso\nde aleatorización) indica que no existen otros factores actuando sobre\n*T*, es decir, *A* es una influencia puramente estocástica. Esta idea y\nel proceso de realización de un experimento controlado con\naleatorización, da cuenta con toda claridad del valor de esta forma de\nrealizar estudios para desentrañar relaciones de causalidad.\n\nEste DAG nos conduce al modelo linear siguiente\n\n$$\ny = \\mu + T +  \\varepsilon\n$$\n\nEn donde *y* son las mediciones de la variable *Y* en respuesta al\nefecto de *T*, $\\mu$ es un valor de referencia general (tradicionalmente\nla media general de la variale *Y*, aunque puede elegirse cualquier otro\nvalor de referencia que convenga al estudio) y la épsilon da cuenta del\nefecto aleatorio inducido por $\\fbox{A}$, por lo que es necesario\npostular una distribución de probabilidades apropiada para caracterizar\nsu comportamiento.\n\n## La tradición de prueba de hipótesis\n\nEl planteamiento de la prueba de significancia estadística de hipótesis\nse pueden encontrar ya en el siglo XIX, su formalización teórica\nrealmente ocurre en los años 20 y 30 del siglo XX con las publicaciones\nde Sir Ronald Fisher, Jerzy Neyman y Egon Pearson.\n\nEntre estos autores existieron diferencias filosóficas y conceptuales\nentre sus planteamientos y posturas. En special Fisher y Neyman\nsostuvieron acres debates que sólo se interrumpieron con el\nfallecimiento de Fisher en 1962. No obstante [el debate continua hasta\nhoy](http://www.elsevier.es/es-revista-investigacion-educacion-medica-343-articulo-la-prueba-hipotesis-nula-sus-X2007505712427368).\n\nEl resultado es que el uso actual de la **prueba estadística de\nhipótesis** se ha conformado como un *extraño híbrido* surgido de una\nmezcla más o menos ecléctica de las dos formas de pensar y no tanto una\nteoría coherente sobre la prueba de hipótesis.\n\n### El procedimiento\n\nEl objetivo de una prueba de significancia es hacer inferencias sobre un\nparámetro que el investigador concibe asociado a un atributo numérico\nrelevante de la población que define su objetivo de investigación. El\nprocedimiento utiliza como base los datos de una muestra extraída de esa\npoblación. El enfoque se opera específicamente como un instrumento para\nexcluir un valor o una gama de valores específicos como plausibles para\nel parámetro.\n\n#### El *paso a paso* del formalismo de prueba de hipótesis\n\n1.  Construir un modelo estadístico. Se trata de un conjunto de\n    supuestos sobre las variables de interés.\n2.  Especificar la hipótesis nula.\n3.  Definir un estadístico de contraste (frecuentemente llamada \"la\n    prueba estadística\").\n4.  Identificar la distribución del estadístico de contraste bajo los\n    supuestos del modelo.\n5.  Calcular, bajo el supuesto de la hipótesis nula, el valor del\n    estadístico de contraste en la muestra observada.\n6.  Calcular la probabilidad de tener un valor del estadístico como el\n    resultante o un valor más extremo en la distribución de referencia\n    (el famosos valor *p*).\n7.  Aceptar o rechazar la hipótesis nula. Si el valor *p* es menor que\n    el criterio α de significancia (especificado a priori), se rechaza\n    la hipótesis nula, en el caso contrario se acepta o por lo menos no\n    se rechaza (por lo pronto).\n\nRechazar la hipótesis nula es algo que quizás produce poca tensión\nemocional, quizás hasta un alivio, finalmente, el investigador sospecha\n(desea mostrar) que lo interesante está en otra parte, en su juego de\nhipótesis alternativas.\n\n### [**¿Es este procedimiento afín al refutacionismo Popperiano?.**]{style=\"color:GoldenRod\"}\n\nPara interpretar correctamente un valor *p* se necesita tener claro que\nse opera dentro de una marco conceptual frecuentista. Esto lleva a que\nse conciba a los parámetros del modelo estadístico como constantes en la\npoblación objetivo (valor fijo que nunca se conoce en realidad).\n\nAdemás se asume que, al menos conceptualmente, sería posible repetir el\nexperimento un número infinito de veces. También se asume que siempre se\nestá muestreando la misma población objetivo (universo muestral) así que\nlos parámetros tienen el mismo valor, pero las muestras fluctúan\naleatoriamente.\n\nBajo estos supuestos es aceptable considerar que el estadístico de\nprueba se distribuye de acuerdo con el modelo de probabilidades\npropuesto para construir el contraste y por lo tanto da cuenta de las\nvariaciones esperadas entre las diferentes repeticiones del experimento.\n\nEn la aproximación tradicional a la *contrastación estadística de\nhipótesis* (frecuentista) se parte de la formulación de proposiciones\nhipotéticas que son descritas con referencia a alguna distribución de\nprobabilidades.\n\nEn este marco conceptual, un componente es la llamada hipótesis nula\n($H_{0}$). Se concibe como un planteamiento que asume la ausencia de\nefecto de los \"factores explicativos\".\n\nEn contraste se propone una o más hipótesis alternativas ($H_{1...n}$),\nen las que se valora algún o algunos efectos de los \"factores\nexplicativos\". La proposición hipotética que hacemos se traduce en\nvalores que podemos comparar con un conjunto de valores a los que\nconsideramos *observados*. La diferencia entre estos dos conjuntos de\nvalores nos permiten valorar la factibilidad de nuestra proposición.\n\n![](images/Potencia_estad%C3%ADstica.png){width=\"700\"}\n\nEn la práctica, suele ocurrir que se concentre la atención en la\nhipótesis nula expresada con la gran simplicidad que implica la\n*ausencia de efectos* y se proceda con menor rigurosidad el análisis de\nla hipótesis alternativa, la que suele procesarse en forma más bien\nexploratoria mediante procedimientos de comparaciones múltiples.\n\n### Críticas\n\nEntre las críticas que se han hecho al procedimiento clásico de\n***prueba de hipótesis*** está la que señala que el valor *p*, al\nexcluir el valor de cero como valor plausible para el parámetro, no\naporta información completa sobre los valores que sí son plausibles.\nEsto implica que la *significancia estadística* no implica *relevancia\npráctica*.\n\n### [**¿Cómo interpretas esta afirmación?**]{style=\"color:GoldenRod\"}\n\nEn el mismo razonamiento, un \"valor de *p* extremadamente significativo\"\nno hace otra cosa que excluir el cero como valor plausible para el\nparámetro no precisamente sobre la calidad del hallazgo.\n\nOtra crítica señala que interpretar el valor *p* en términos de\n*evidencia en contra de la hipótesis nula* (siguiendo el pensamiento de\nFisher) o *la plausibilidad de que la hipótesis nula sea falsa* a veces\nse expresan equivocadamente como la probabilidad de que la hipótesis\nnula sea falsa en consideración de la evidencia (*E*) disponible. Al\nplantearlo así, formalmente se enuncia como $P(H_{0}|E)$. Pero esto no\nes apropiado, formalmente resulta ser una inconsistencia en la lógica\ndel planteamiento.\n\n### [**¿Puedes reconocer esta inconsistencia?**]{style=\"color:GoldenRod\"}\n\nLa inconsistencia está en que, en primer lugar como dije arriba, el\nvalor *p* se define dentro del marco frecuentista y se concibe que los\nparámetros son valores constantes, aunque desconocidos (¡supuesto\nestadístico de efectos fijos!). No se trata de los parámetros de alguna\ndistribución de probabilidades (observada o no). Por tanto, no tiene\nsentido asignar probabilidades a los distintos valores estimados del\nparámetro.\n\nAdemás, el valor *p* se calcula bajo el supuesto de que la hipótesis\nnula es cierta; esto hace imposible, por construcción, interpretarlo\ncomo la probabilidad de que la hipótesis alternativa sea cierta. La\nprobabilidad a la que se refiere el valor *p* guarda más relación con la\nprobabilidad inversa, $P(E|H{0})$. ¿Qué tan probable sería tener una\nmuestra como la que tenemos enfrente, si la hipótesis considerada fuera\ncierta?. Esto se conoce como la **verosimilitud**, es decir, la\nprobabilidad de observar los datos que se han obtenido en un estudio\nsuponiendo que los atributos del modelo fueran ciertos. La verosimilitud\nvalora a la muestra como un resultado *condicional* a los supuestos\nhechos en el modelo estadístico y en este caso, la hipótesis nula.\n\nSin embargo, la probabilidad que realmente interesa -por ejemplo, al\ninvestigador de nuestro ejemplo- es la anteriormente mencionada\n$P(H_{0}|E)$. Aunque no está definida dentro del marco frecuentista, en\nel marco Bayesiano sí se define. Las probabilidades $P(E|H_{0})$ y\n$P(H_{0}|E)$ no son iguales.\n\n### [**¿Recuerdas qué representa cada uno de ellas? ¿cuál es la relación entre ambas?**]{style=\"color:GoldenRod\"}\n\nOtra crítica interesante surge de la llamada paradoja de Lindley (1957),\nquien mostró, con una formulación Bayesiana, que existe la posibilidad\nde tener datos congruentes con rechazar una hipótesis nula con un bajo\nvalor *p* y que al mismo tiempo llevan a una probabilidad posterior\nalta.\n\nEncontró que es perfectamente posible, a partir de los mismos datos *E*,\nobtener al mismo tiempo una $P(E|H_{0})$ = 0.05 (baja probabilidad de\nobtener una muestra como la que se observó, si $H_{0}$ fuera cierta) y\n$P(H_{0}|E)$ = 0.95 (fuerte evidencia en favor de $H_{0}$). Este\nresultado contradictorio permite ver lo cuestionable que resulta\ninterpretar el **valor *p* como evidencia en contra de la hipótesis\nnula.**\n\nSe ha contra argumentado que la paradoja requiere muestras grandes para\nmanifestarse, y se oponen a los supuestos adicionales que requiere el\nanálisis Bayesiano. Se ha defendido que bajo condiciones razonables, un\nbajo valor *p* generalmente implica una baja probabilidad posterior, es\ndecir, poca evidencia para la hipótesis nula. Sin embargo, a pesar de\nesta defensa al enfoque *clásico*, se ha encontrado que los valores *p*\nsistemáticamente sobrestiman la evidencia en contra de la hipótesis\nnula.\n\nEn resumen, el valor mismo de *p*, resultado de una prueba clásica de\nhipótesis, no aporta mucha información de interés para los\ninvestigadores. En caso de optar por la hipótesis alternativa con base\nen la *p*, no se favorece llegar a ninguna conclusión sustancial sobre\nposibles explicaciones alternativas, lo único que queda claro es que la\nnula probablemente es falsa.\n\nPara que quede claro, hay que insistir en que si el valor *p* es juzgado\n***significativo***, únicamente nos inclina a **excluir un solo valor\ncomo estimador plausible** para el parámetro. Peor aún, el significado\nde **plausible** en la última expresión tiene una relación nebulosa con\nla probabilidad que sí le interesa a los investigadores: ***la\nprobabilidad posterior de que la hipótesis nula sea cierta a la luz de\nla evidencia recopilada*** $P(H_{0}|E)$.\n\n::: {.callout-tip title=\"¿Qué piensas de la paradoja de Lindley y sus implicaciones\"}\n![](images/bullet-2728144_1280.jpg){width=\"50%\"}\n:::\n\n### Remedios y alternativas para la prueba estadística de la hipótesis nula\n\nPara enfrentar algunos de los inconvenientes del enfoque clásico de\nprueba de hipótesis se ha recomendado ahora sustituir el valor *p* por\nun *intervalo de confianza* que abarca un conjunto de valores que\npermiten valorar si es razonable rechazar la hipótesis nula y además, en\ncaso contrario proporciona una gama de valores que caracterizan al\nparámetro, lo que resulta de mucho interés.\n\nLa práctica de presentar intervalos de confianza, posiblemente en\nconjunto con *p*, constituye una respuesta a la crítica de que sólo se\nexcluye un valor como valor plausible para el parámetro. Además, hacer\nesto proporciona información sobre significancia. Si el intervalo no\nincluye el valor de cero, entonces se declararía el resultado como\n**estadísticamente significativo**. El intervalo informa también sobre\nel posible tamaño del efecto.\n\nA la luz de las críticas, Muchos autores ven necesario actualmente\nadoptar el marco Bayesiano para enfrentar las deficiencias del enfoque\n*clásico*.\n\n### Potencia de la prueba\n\nEn la práctica, la potencia de la prueba depende del *grado de\ndispersión* en los datos. Si se está asumiendo un modelo de\nprobabilidades Gaussiano (distribución normal), el factor de dispersión\no escala se relaciona con la varianza. Por lo tanto, es usual notar que\nel mismo cálculo del error estandar, $s_{e}=\\frac{\\sigma}{\\sqrt{n}}$,\nsugiere la solución.\n\n![](images/Potencia_estad%C3%ADstica.png){width=\"700\"}\n\n1.  Se puede incrementar el tamaño de muestra, *n*,\n\n::: {.callout-tip title=\"¿Por qué funcionaría esto?\"}\n![](images/geometry-1922743_1280.jpg){width=\"50%\"}\n:::\n\n2.  Aumentar la precisión con la que se estima $\\sigma^2$,\n\n::: {.callout-tip title=\"¿Cómo se puede hacer esto?\"}\n![](images/key-5216637_1280.jpg){width=\"50%\"}\n:::\n\nEs interesante apreciar, que la búsqueda de un tamaño de muestra\napropiado para un estudio que estemos planeando, se puede lograr muy\neficazmente haciendo simulaciones como las que hemos estado viendo en\neste bloque del curso. A través de este camino y haciendo el esfuerzo de\nespecificar hipótesis alternativas relevantes se pueden resolver\npreguntas como:\n\n-   ¿Cuál es el tamaño de muestra necesario para detectar una cierta\n    diferencia en lo que medimos?\n-   ¿Cuál es la diferencia detectable dada una *n* o una potencia de la\n    prueba ($1-\\beta$)?\n-   ¿Cuál es la potencia ($1-\\beta$) dado un *n* y cierta diferencia con\n    $H_{a}$ de interés?\n\n# Participación activa\n\nHemos previsto tener frecuentes eventos interactivos para dar\nseguimiento a los temas que iremos tratando o para generar espacios de\nenfoque que nos ayuden a generar contextos de discusión que nos ayuden a\nreflexionar sobre cómo es que hacemos ciencia. Para hacer esto\nutilizaremos la aplicación en línea a la que podrán acceder desde sus\ncomputadoras o teléfonos celulares:\n\n![](images/vevox-QR.png){width=\"400\"}\n\n## Quiz: Prueba estadística de hipótesis\n\nParticipa: vevox.app ID: 125-688-362\n\n::: {layout=\"[[-1, 80, -10]]\" layout-valign=\"top\"}\n[![](images/clase_1-conceptos-prueba-hipotesis.png){width=\"200\"}](https://vevox.app/#/m/125688362)\n:::\n\n\\\n\n\n## Disposición de tratamientos e intereses sobre los factores\n\n### Efectos fijos\n\nQuizás la forma más simple de identificar las variables explicativas que\ntienen efectos fijos es pensar en ellas como variables cuyos niveles\nidentifican en forma completa las condiciones de interés para el\ninvestigador. Por ejemplo, en el caso de un experimento que analiza el\ndesempeño de larvas de mariposa que toman una dieta rica en proteinas y\nal mismo tiempo están expuestas a la presencia o no de un alcaloide.\nEstamos interesados precisamente en esas dietas y en la presencia o no\ndel alcaloide. Estos dos factores son fijos. Es el tipo de variables que\nnormalmente consideramos en nuestros objetivos de investigación. Se\nasume que su identificación y definición es completa, se asume que no\nhay más niveles de interés que los definidos y por lo tanto el modelo\nresultante no puede utilizarse para predecir fuera del ámbito de esas\ndefiniciones.\n\n### Efectos aleatorios\n\nLas variables de efectos aleatorios surgen cuando se considera que el\nfactor considerado no es sino una muestra de los posibles resultados que\nse pueden obtener de muestrear la condición que caracteriza el factor.\nPor ejemplo, si en un experimento para explorar la germinación de\n*Bouteloua gracilis* bajo distintas condiciones de temperatura en campo,\nse distribuyen las semillas en varios sitios de una zona de interés. Los\naprecia que hay básicamente dos tipos de ambiente, suelos arenosos y\nsuelos con algo de grava, así que se eligen 5 sitios en cada condición,\ny en cada uno de ellos se ponen a prueba dos tratamientos, \"pisoteo por\nganado\" y \"sin pisoteo por ganado\". Los 10 sitios elegidos estarían\ndefinidos como de efectos aleatorios, pues podemos ver que los niveles\nelegidos son en realidad una muestra de las posibles condiciones que\nprevalecen en la zona de estudio. Además, claramente el interés de la\npredicción es ser generalizable para toda la zona. A veces podemos\npensar en esta forma de proceder como equivalente a un *muestreo\nestratificado*, en este caso, los tipos de ambiente son los estratos. No\nes el caso del tratamiento pisoteo. aprovechando podemos ver que en este\nexperimento tendremos un mínimo de 4 combinaciones experimentales, y que\nese arreglo mínimo se repetirá 5 veces, así que reqeriremos 20 unidades\nexperimentales para realizar el estudio.\n\n::: {.callout-tip title=\"¿Puedes darnos un ejemplo en el que distingas entre efectos fijos y aleatorios?\"}\n![](images/man-159771_1280.png){width=\"50%\"}\n:::\n\n### Anidamiento vs. cruzamiento\n\nLa anidación o el cruzamiento es otra característica de los datos, o más\nbien del diseño experimental. Hablamos de que un conjunto de variables\nestán cruzadas en un diseño experimental cuando todos los posibles\nniveles de las variables están expuestas por igual entre ellas.\nPodríamos decir que las variables se combinan de \"igual a igual\". Es\ndecir, podemos tener tantas posibles combinaciones de las variables como\nel producto del numero de niveles que tengan. En el caso del experimento\nde *Bouteloua*, el experimento sugiere que los sitios y los tratamientos\nestán \"cruzados\", de ahí que tengamos necesidad de disponer por lo menos\nde 4 unidades experimentales.\n\nEl ejemplo de escuelas que ilustro a continuación debe ayudar a entender\nmejor estos conceptos. Si las clases son iguales para todas las\nescuelas, nos estaríamos refiriendo a algo así:\n\n![](images/Bx8v2.png)\n\nEsto significa que cada clase se imparte por igual y en las mismas\ncondiciones a cada escuela. Algo difícil de imaginar, ¡pero quizás no en\nlos tiempos de la COVID-19!. Este es un diseño cruzado (algunos también\npodrían llamarlo afiliación múltiple). En **R** y con las funciones que\najustan modelos estadísticos lineales (`lm() y glm()`) se produce\nmediante el operador `*`.\n\nEl arreglo anidado se produce cuando las unidades experimentales están\nsubordinadas a algún criterio de clasificación. Un factor B está anidado\nen otro factor A cuando cada nivel del factor B aparece asociado a un\núnico nivel del factor A (los niveles de B están subordinados a los de\nA). Aquí tenemos clases anidadas en escuelas, lo cual es un escenario\nfamiliar.\n\n![](images/ahXIG.png)\n\nEl punto importante aquí es que, entre cada escuela, las clases tienen\nel mismo identificador, aunque sean distintas si están anidadas. La\nclase 1 aparece en la escuela 1, la escuela 2 y la escuela 3. Sin\nembargo, si los datos están anidados, la clase 1 en la escuela 1 no es\nla misma unidad de medida que la clase 1 en la escuela 2 y la escuela 3.\n\nNo es posible saber, simplemente inspeccionando los datos, si tenemos\nefectos aleatorios anidados o cruzados. Esto sólo puede determinarse con\nel conocimiento de los datos y el diseño experimental. Debido a esto, es\nmuy importante especificar con suficiente claridad el diseño\nexperimental incluyendo las operaciones involucradas para ponerlo en\npráctica, para poder construir correctamente el modelo estadístico\ncorrespondiente, ya que dependiendo de la naturaleza de las variables\n(fija o aleatoria), los modelos producirán resultados diferentes.\n\nEl concepto de variables aleatorias no es fácil de comprender, por lo\nque no hay que preocuparse demasiado por entenderlo completamente en\neste momento. También es útil tener en cuenta que el hecho de que una\nvariable se considere fija o aleatoria en cierto grado dependerá de la\ninterpretación de la persona que diseña el experimento y realiza el\nanálisis. En **R**, la operación para incluir efectos anidados es el\noperador `/`.\n\nEn forma específica las interacciones derivadas de cruzamiento se pueden\nanotar en un modelo como `a:b` y un anidamiento `a %in% b`\n\nNaturalmente podemos encontrar situaciones en las que el experimento\ncombina efectos aleatorios y fijos. Naturalmente, tal diseño se denomina\nde efectos mixtos.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}